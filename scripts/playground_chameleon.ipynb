{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoXn14ltnGh3"
      },
      "source": [
        "# Using an SAE as a steering vector\n",
        "\n",
        "This notebook demonstrates how to use SAE lens to identify a feature on a pretrained model, and then construct a steering vector to affect the models output to various prompts. This notebook will also make use of Neuronpedia for identifying features of interest.\n",
        "\n",
        "The steps below include:\n",
        "\n",
        "\n",
        "\n",
        "*   Installing relevant packages (Colab or locally)\n",
        "*   Load your SAE and the model it used\n",
        "*   Determining your feature of interest and its index\n",
        "*   Implementing your steering vector\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf3lJYPEXh0v"
      },
      "source": [
        "## Setting up packages and notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9k5iGyOXtuN"
      },
      "source": [
        "### Import and installs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fapxk8MDrs6R"
      },
      "source": [
        "#### Environment Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0TwNmRkRUgR7",
        "outputId": "ffeb827a-9af2-4b09-b8dd-78e0d594ddf6"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  # for google colab users\n",
        "    import google.colab # type: ignore\n",
        "    from google.colab import output\n",
        "    COLAB = True\n",
        "    %pip install sae-lens transformer-lens\n",
        "except:\n",
        "  # for local setup\n",
        "    COLAB = False\n",
        "    from IPython import get_ipython # type: ignore\n",
        "    ipython = get_ipython(); assert ipython is not None\n",
        "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
        "    ipython.run_line_magic(\"autoreload\", \"2\")\n",
        "\n",
        "# Imports for displaying vis in Colab / notebook\n",
        "import webbrowser\n",
        "import http.server\n",
        "import socketserver\n",
        "import threading\n",
        "PORT = 8000\n",
        "\n",
        "# general imports\n",
        "import os\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import plotly.express as px\n",
        "\n",
        "torch.set_grad_enabled(False);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NGgIu1ZVYDub"
      },
      "outputs": [],
      "source": [
        "def display_vis_inline(filename: str, height: int = 850):\n",
        "    '''\n",
        "    Displays the HTML files in Colab. Uses global `PORT` variable defined in prev cell, so that each\n",
        "    vis has a unique port without having to define a port within the function.\n",
        "    '''\n",
        "    if not(COLAB):\n",
        "        webbrowser.open(filename);\n",
        "\n",
        "    else:\n",
        "        global PORT\n",
        "\n",
        "        def serve(directory):\n",
        "            os.chdir(directory)\n",
        "\n",
        "            # Create a handler for serving files\n",
        "            handler = http.server.SimpleHTTPRequestHandler\n",
        "\n",
        "            # Create a socket server with the handler\n",
        "            with socketserver.TCPServer((\"\", PORT), handler) as httpd:\n",
        "                print(f\"Serving files from {directory} on port {PORT}\")\n",
        "                httpd.serve_forever()\n",
        "\n",
        "        thread = threading.Thread(target=serve, args=(\"/content\",))\n",
        "        thread.start()\n",
        "\n",
        "        output.serve_kernel_port_as_iframe(PORT, path=f\"/{filename}\", height=height, cache_in_notebook=True)\n",
        "\n",
        "        PORT += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmaPYLpGrxbo"
      },
      "source": [
        "#### General Installs and device setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdUm9rZKr1Qb",
        "outputId": "9b73b762-1356-437b-8925-91c514093b43"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/saev/miniconda3/envs/hantao_interp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda:7\n"
          ]
        }
      ],
      "source": [
        "# package import\n",
        "from torch import Tensor\n",
        "from transformer_lens import utils\n",
        "from functools import partial\n",
        "from jaxtyping import Int, Float\n",
        "\n",
        "# device setup\n",
        "if torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cuda:7\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsB0qORUaXiK"
      },
      "source": [
        "### Load your model and SAE\n",
        "\n",
        "We're going to work with a pretrained GPT2-small model, and the RES-JB SAE set which is for the residual stream."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bCvNtm1OOhlR",
        "outputId": "e6fd27ab-ee94-46ec-a07e-ee48c8f30da3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.02s/it]\n"
          ]
        }
      ],
      "source": [
        "from transformer_lens import HookedChameleon\n",
        "from sae_lens import SAE\n",
        "from sae_lens.toolkit.pretrained_saes import get_gpt2_res_jb_saes\n",
        "from transformers import ChameleonForConditionalGeneration\n",
        "# Choose a layer you want to focus on\n",
        "# For this tutorial, we're going to use layer 2\n",
        "layer = 16\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3,4,5,6,7'\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "local_model = ChameleonForConditionalGeneration.from_pretrained(\"/home/saev/hantao/models/Anole-7b-v0.1-hf\")\n",
        "\n",
        "# get model\n",
        "model = HookedChameleon.from_pretrained(\"htlou/AA-Chameleon-7B-plus\", hf_model = local_model, n_devices = 6)\n",
        "\n",
        "# # get the SAE for this layer\n",
        "# sae, cfg_dict, _ = SAE.from_pretrained(\n",
        "#     release = \"htlou/AA-Chameleon-7B-plus-res-jb\",\n",
        "#     sae_id = f\"blocks.{layer}.hook_resid_post\",\n",
        "#     device = device\n",
        "# )\n",
        "sae = SAE.load_from_pretrained(\n",
        "    path = \"/home/saev/hantao/SAELens-V/scripts/checkpoints/1018_obelics_10k/final_122880000\",\n",
        "    device = device\n",
        ")\n",
        "\n",
        "# get hook point\n",
        "hook_point = sae.cfg.hook_name\n",
        "print(hook_point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([10, 2048])\n"
          ]
        }
      ],
      "source": [
        "from transformer_lens.utils import tokenize_and_concatenate\n",
        "from datasets import load_dataset\n",
        "\n",
        "# dataset = load_dataset(\n",
        "#     path = \"NeelNanda/pile-10k\",\n",
        "#     split=\"train\",\n",
        "#     streaming=False,\n",
        "# )\n",
        "\n",
        "# token_dataset = tokenize_and_concatenate(\n",
        "#     dataset= dataset,# type: ignore\n",
        "#     tokenizer = model.tokenizer, # type: ignore\n",
        "#     streaming=True,\n",
        "#     max_length=sae.cfg.context_size,\n",
        "#     add_bos_token=sae.cfg.prepend_bos,\n",
        "# )\n",
        "\n",
        "token_dataset = load_dataset(\n",
        "    path = \"/home/saev/hantao/data/obelics_obelics_10k_tokenized_2048\",\n",
        "    split = \"train\",\n",
        "    streaming=False\n",
        ")\n",
        "\n",
        "data_list = []\n",
        "for item in token_dataset:\n",
        "    data_list.append(item[\"input_ids\"])\n",
        "\n",
        "batch_tokens = torch.tensor(data_list[:10])\n",
        "# batch_tokens = torch.tensor(token_dataset[:1]['tokens'])\n",
        "print(batch_tokens.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 5.00 GiB. GPU 0 has a total capacity of 79.14 GiB of which 4.06 GiB is free. Including non-PyTorch memory, this process has 75.07 GiB memory in use. Of the allocated memory 72.62 GiB is allocated by PyTorch, and 1.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m sae\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# prevents error if we're expecting a dead neuron mask for who grads\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# activation store can give us tokens.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# batch_tokens = token_dataset[:5][\"tokens\"]\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# batch_tokens = inputs\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     _, cache \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mrun_with_cache(batch_tokens, prepend_bos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Use the SAE\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     feature_acts \u001b[38;5;241m=\u001b[39m sae\u001b[38;5;241m.\u001b[39mencode(cache[sae\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mhook_name]\u001b[38;5;241m.\u001b[39mto(device))\n",
            "File \u001b[0;32m~/hantao/TransformerLens-V/transformer_lens/HookedChameleon.py:670\u001b[0m, in \u001b[0;36mHookedChameleon.run_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_with_cache\u001b[39m(\n\u001b[1;32m    654\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mmodel_args, return_cache_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, remove_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    655\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    662\u001b[0m     Union[ActivationCache, Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]],\n\u001b[1;32m    663\u001b[0m ]:\n\u001b[1;32m    664\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper around `run_with_cache` in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    665\u001b[0m \n\u001b[1;32m    666\u001b[0m \u001b[38;5;124;03m    If return_cache_object is True, this will return an ActivationCache object, with a bunch of\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;124;03m    useful HookedTransformer specific methods, otherwise it will return a dictionary of\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;124;03m    activations as in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 670\u001b[0m     out, cache_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrun_with_cache(\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;241m*\u001b[39mmodel_args, remove_batch_dim\u001b[38;5;241m=\u001b[39mremove_batch_dim, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    672\u001b[0m     )\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_cache_object:\n\u001b[1;32m    674\u001b[0m         cache \u001b[38;5;241m=\u001b[39m ActivationCache(cache_dict, \u001b[38;5;28mself\u001b[39m, has_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m remove_batch_dim)\n",
            "File \u001b[0;32m~/hantao/TransformerLens-V/transformer_lens/hook_points.py:566\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, pos_slice, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m cache_dict, fwd, bwd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_caching_hooks(\n\u001b[1;32m    553\u001b[0m     names_filter,\n\u001b[1;32m    554\u001b[0m     incl_bwd,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    557\u001b[0m     pos_slice\u001b[38;5;241m=\u001b[39mpos_slice,\n\u001b[1;32m    558\u001b[0m )\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks(\n\u001b[1;32m    561\u001b[0m     fwd_hooks\u001b[38;5;241m=\u001b[39mfwd,\n\u001b[1;32m    562\u001b[0m     bwd_hooks\u001b[38;5;241m=\u001b[39mbwd,\n\u001b[1;32m    563\u001b[0m     reset_hooks_end\u001b[38;5;241m=\u001b[39mreset_hooks_end,\n\u001b[1;32m    564\u001b[0m     clear_contexts\u001b[38;5;241m=\u001b[39mclear_contexts,\n\u001b[1;32m    565\u001b[0m ):\n\u001b[0;32m--> 566\u001b[0m     model_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m incl_bwd:\n\u001b[1;32m    568\u001b[0m         model_out\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[0;32m~/miniconda3/envs/hantao_interp/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/envs/hantao_interp/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/hantao/TransformerLens-V/transformer_lens/HookedChameleon.py:580\u001b[0m, in \u001b[0;36mHookedChameleon.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    578\u001b[0m         output_attentions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (output_attention,)\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 580\u001b[0m         residual \u001b[38;5;241m=\u001b[39m block(\n\u001b[1;32m    581\u001b[0m             residual,\n\u001b[1;32m    582\u001b[0m             past_kv_cache_entry\u001b[38;5;241m=\u001b[39mpast_kv_cache[i] \u001b[38;5;28;01mif\u001b[39;00m past_kv_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    583\u001b[0m             shortformer_pos_embed\u001b[38;5;241m=\u001b[39mshortformer_pos_embed,\n\u001b[1;32m    584\u001b[0m             attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    585\u001b[0m         )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattentions\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "File \u001b[0;32m~/miniconda3/envs/hantao_interp/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/envs/hantao_interp/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/hantao/TransformerLens-V/transformer_lens/components/transformer_block.py:179\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    164\u001b[0m     attn_out, output_attention \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    176\u001b[0m         )\n\u001b[1;32m    177\u001b[0m     )  \u001b[38;5;66;03m# [batch, pos, d_model], [batch, head_index, pos, kv_pos]\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\n\u001b[1;32m    180\u001b[0m         query_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(query_input)\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m shortformer_pos_embed),\n\u001b[1;32m    182\u001b[0m         key_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(key_input)\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m shortformer_pos_embed),\n\u001b[1;32m    184\u001b[0m         value_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(value_input),\n\u001b[1;32m    185\u001b[0m         past_kv_cache_entry\u001b[38;5;241m=\u001b[39mpast_kv_cache_entry,\n\u001b[1;32m    186\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    187\u001b[0m     )\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_normalization_before_and_after:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# If we use LayerNorm both before and after, then apply the second LN after the layer\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;66;03m# and before the hook. We do it before the hook so hook_attn_out captures \"that which\u001b[39;00m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;66;03m# is added to the residual stream\"\u001b[39;00m\n\u001b[1;32m    192\u001b[0m     attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1_post(attn_out)\n",
            "File \u001b[0;32m~/miniconda3/envs/hantao_interp/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/envs/hantao_interp/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/hantao/TransformerLens-V/transformer_lens/components/abstract_attention.py:258\u001b[0m, in \u001b[0;36mAbstractAttention.forward\u001b[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask, position_bias)\u001b[0m\n\u001b[1;32m    256\u001b[0m attn_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_attn_scores(attn_scores)\n\u001b[1;32m    257\u001b[0m pattern \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(attn_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 258\u001b[0m pattern \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(torch\u001b[38;5;241m.\u001b[39misnan(pattern), torch\u001b[38;5;241m.\u001b[39mzeros_like(pattern), pattern)\n\u001b[1;32m    259\u001b[0m pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_pattern(pattern)  \u001b[38;5;66;03m# [batch, head_index, query_pos, key_pos]\u001b[39;00m\n\u001b[1;32m    260\u001b[0m pattern \u001b[38;5;241m=\u001b[39m pattern\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdtype)\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.00 GiB. GPU 0 has a total capacity of 79.14 GiB of which 4.06 GiB is free. Including non-PyTorch memory, this process has 75.07 GiB memory in use. Of the allocated memory 72.62 GiB is allocated by PyTorch, and 1.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "sae.eval()  # prevents error if we're expecting a dead neuron mask for who grads\n",
        "\n",
        "with torch.no_grad():\n",
        "    # activation store can give us tokens.\n",
        "    # batch_tokens = token_dataset[:5][\"tokens\"]\n",
        "    # batch_tokens = inputs\n",
        "    _, cache = model.run_with_cache(batch_tokens, prepend_bos=True)\n",
        "\n",
        "    # Use the SAE\n",
        "    \n",
        "    feature_acts = sae.encode(cache[sae.cfg.hook_name].to(device))\n",
        "    sae_out = sae.decode(feature_acts)\n",
        "\n",
        "    # save some room\n",
        "    del cache\n",
        "\n",
        "    # ignore the bos token, get the number of features that activated in each token, averaged accross batch and position\n",
        "    l0 = (feature_acts[:, 1:] > 0).float().sum(-1).detach()\n",
        "    print(\"average l0\", l0.mean().item())\n",
        "    px.histogram(l0.flatten().cpu().numpy()).show()\n",
        "\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([10, 2048])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# print infinity length\n",
        "torch.set_printoptions(threshold=torch.inf)\n",
        "batch_tokens.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Orig 16.37755012512207\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reconstr 29.15433120727539\n",
            "Zero 11.090354919433594\n"
          ]
        }
      ],
      "source": [
        "from transformer_lens import utils\n",
        "from functools import partial\n",
        "torch.cuda.empty_cache()\n",
        "# next we want to do a reconstruction test.\n",
        "def reconstr_hook(activation, hook, sae_out):\n",
        "    return sae_out\n",
        "\n",
        "\n",
        "def zero_abl_hook(activation, hook):\n",
        "    return torch.zeros_like(activation)\n",
        "\n",
        "\n",
        "print(\"Orig\", model(batch_tokens, return_type=\"loss\").item())\n",
        "print(\n",
        "    \"reconstr\",\n",
        "    model.run_with_hooks(\n",
        "        batch_tokens,\n",
        "        fwd_hooks=[\n",
        "            (\n",
        "                sae.cfg.hook_name,\n",
        "                partial(reconstr_hook, sae_out=sae_out),\n",
        "            )\n",
        "        ],\n",
        "        return_type=\"loss\",\n",
        "    ).item(),\n",
        ")\n",
        "print(\n",
        "    \"Zero\",\n",
        "    model.run_with_hooks(\n",
        "        batch_tokens,\n",
        "        return_type=\"loss\",\n",
        "        fwd_hooks=[(sae.cfg.hook_name, zero_abl_hook)],\n",
        "    ).item(),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters\n",
        "top_n = 5  # 设定每个特征保留的top激活数量\n",
        "\n",
        "# 初始化存储每个特征top-n激活值的字典\n",
        "feature_top_activations = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_top_n_activations(feature_activations, feature_top_activations, top_n=5):\n",
        "    \"\"\"\n",
        "    更新每个特征的top-n激活值.\n",
        "    \n",
        "    feature_activations: 当前数据的特征激活值\n",
        "    feature_top_activations: 存储top-n激活值的字典\n",
        "    top_n: 要保留的top激活数量\n",
        "    \"\"\"\n",
        "    for feature_idx, activation_value in enumerate(feature_activations):\n",
        "        if feature_idx not in feature_top_activations:\n",
        "            feature_top_activations[feature_idx] = []\n",
        "        feature_top_activations[feature_idx].append(activation_value)\n",
        "        # 仅保留top-n激活值\n",
        "        feature_top_activations[feature_idx] = sorted(\n",
        "            feature_top_activations[feature_idx], reverse=True)[:top_n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 5.00 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.25 GiB is free. Including non-PyTorch memory, this process has 77.88 GiB memory in use. Of the allocated memory 76.37 GiB is allocated by PyTorch, and 1.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 15\u001b[0m\n\u001b[1;32m      6\u001b[0m test_feature_idx_gpt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m)) \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m14057\u001b[39m]\n\u001b[1;32m      8\u001b[0m feature_vis_config_gpt \u001b[38;5;241m=\u001b[39m SaeVisConfig(\n\u001b[1;32m      9\u001b[0m     hook_point\u001b[38;5;241m=\u001b[39mhook_point,\n\u001b[1;32m     10\u001b[0m     features\u001b[38;5;241m=\u001b[39mtest_feature_idx_gpt,\n\u001b[1;32m     11\u001b[0m     minibatch_size_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     12\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 15\u001b[0m sae_vis_data_gpt \u001b[38;5;241m=\u001b[39m SaeVisData\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     16\u001b[0m     encoder\u001b[38;5;241m=\u001b[39msae,\n\u001b[1;32m     17\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel, \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     tokens\u001b[38;5;241m=\u001b[39mbatch_tokens,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     cfg\u001b[38;5;241m=\u001b[39mfeature_vis_config_gpt,\n\u001b[1;32m     20\u001b[0m )\n",
            "File \u001b[0;32m~/miniconda3/envs/hantao_interp/lib/python3.11/site-packages/sae_vis/data_storing_fns.py:1037\u001b[0m, in \u001b[0;36mSaeVisData.create\u001b[0;34m(cls, encoder, model, tokens, cfg, encoder_B)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1035\u001b[0m     encoder_wrapper \u001b[38;5;241m=\u001b[39m encoder\n\u001b[0;32m-> 1037\u001b[0m sae_vis_data \u001b[38;5;241m=\u001b[39m get_feature_data(\n\u001b[1;32m   1038\u001b[0m     encoder\u001b[38;5;241m=\u001b[39mencoder_wrapper,\n\u001b[1;32m   1039\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1040\u001b[0m     tokens\u001b[38;5;241m=\u001b[39mtokens,\n\u001b[1;32m   1041\u001b[0m     cfg\u001b[38;5;241m=\u001b[39mcfg,\n\u001b[1;32m   1042\u001b[0m     encoder_B\u001b[38;5;241m=\u001b[39mencoder_B,\n\u001b[1;32m   1043\u001b[0m )\n\u001b[1;32m   1044\u001b[0m sae_vis_data\u001b[38;5;241m.\u001b[39mcfg \u001b[38;5;241m=\u001b[39m cfg\n\u001b[1;32m   1045\u001b[0m sae_vis_data\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n",
            "File \u001b[0;32m~/miniconda3/envs/hantao_interp/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/envs/hantao_interp/lib/python3.11/site-packages/sae_vis/data_fetching_fns.py:613\u001b[0m, in \u001b[0;36mget_feature_data\u001b[0;34m(encoder, model, tokens, cfg, encoder_B)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;66;03m# For each batch of features: get new data and update global data storage objects\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m features \u001b[38;5;129;01min\u001b[39;00m feature_batches:\n\u001b[0;32m--> 613\u001b[0m     new_feature_data, new_time_logs \u001b[38;5;241m=\u001b[39m _get_feature_data(\n\u001b[1;32m    614\u001b[0m         encoder, encoder_B, model_wrapper, tokens, features, cfg, progress\n\u001b[1;32m    615\u001b[0m     )\n\u001b[1;32m    616\u001b[0m     sae_vis_data\u001b[38;5;241m.\u001b[39mupdate(new_feature_data)\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m new_time_logs\u001b[38;5;241m.\u001b[39mitems():\n",
            "File \u001b[0;32m~/miniconda3/envs/hantao_interp/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/envs/hantao_interp/lib/python3.11/site-packages/sae_vis/data_fetching_fns.py:491\u001b[0m, in \u001b[0;36m_get_feature_data\u001b[0;34m(encoder, encoder_B, model, tokens, feature_indices, cfg, progress)\u001b[0m\n",
            "File \u001b[0;32m~/miniconda3/envs/hantao_interp/lib/python3.11/site-packages/sae_vis/model_fns.py:198\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, tokens, return_logits)\u001b[0m\n",
            "File \u001b[0;32m~/hantao/TransformerLens-V/transformer_lens/hook_points.py:454\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_hooks\u001b[0;34m(self, fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARNING: Hooks will be reset at the end of run_with_hooks. This removes the backward hooks before a backward pass can occur.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m     )\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks(fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts) \u001b[38;5;28;01mas\u001b[39;00m hooked_model:\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hooked_model\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n",
            "File \u001b[0;32m~/hantao/TransformerLens-V/transformer_lens/HookedChameleon.py:580\u001b[0m, in \u001b[0;36mHookedChameleon.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    578\u001b[0m         output_attentions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (output_attention,)\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 580\u001b[0m         residual \u001b[38;5;241m=\u001b[39m block(\n\u001b[1;32m    581\u001b[0m             residual,\n\u001b[1;32m    582\u001b[0m             past_kv_cache_entry\u001b[38;5;241m=\u001b[39mpast_kv_cache[i] \u001b[38;5;28;01mif\u001b[39;00m past_kv_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    583\u001b[0m             shortformer_pos_embed\u001b[38;5;241m=\u001b[39mshortformer_pos_embed,\n\u001b[1;32m    584\u001b[0m             attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    585\u001b[0m         )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattentions\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "File \u001b[0;32m~/miniconda3/envs/hantao_interp/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/envs/hantao_interp/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/hantao/TransformerLens-V/transformer_lens/components/transformer_block.py:179\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    164\u001b[0m     attn_out, output_attention \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    176\u001b[0m         )\n\u001b[1;32m    177\u001b[0m     )  \u001b[38;5;66;03m# [batch, pos, d_model], [batch, head_index, pos, kv_pos]\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\n\u001b[1;32m    180\u001b[0m         query_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(query_input)\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m shortformer_pos_embed),\n\u001b[1;32m    182\u001b[0m         key_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(key_input)\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m shortformer_pos_embed),\n\u001b[1;32m    184\u001b[0m         value_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(value_input),\n\u001b[1;32m    185\u001b[0m         past_kv_cache_entry\u001b[38;5;241m=\u001b[39mpast_kv_cache_entry,\n\u001b[1;32m    186\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    187\u001b[0m     )\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_normalization_before_and_after:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# If we use LayerNorm both before and after, then apply the second LN after the layer\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;66;03m# and before the hook. We do it before the hook so hook_attn_out captures \"that which\u001b[39;00m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;66;03m# is added to the residual stream\"\u001b[39;00m\n\u001b[1;32m    192\u001b[0m     attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1_post(attn_out)\n",
            "File \u001b[0;32m~/miniconda3/envs/hantao_interp/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/envs/hantao_interp/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/hantao/TransformerLens-V/transformer_lens/components/abstract_attention.py:216\u001b[0m, in \u001b[0;36mAbstractAttention.forward\u001b[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask, position_bias)\u001b[0m\n\u001b[1;32m    213\u001b[0m     q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    214\u001b[0m     k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m--> 216\u001b[0m attn_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_attention_scores(\n\u001b[1;32m    217\u001b[0m     q, k\n\u001b[1;32m    218\u001b[0m )  \u001b[38;5;66;03m# [batch, head_index, query_pos, key_pos]\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mpositional_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malibi\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    221\u001b[0m     query_ctx \u001b[38;5;241m=\u001b[39m attn_scores\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
            "File \u001b[0;32m~/hantao/TransformerLens-V/transformer_lens/components/abstract_attention.py:563\u001b[0m, in \u001b[0;36mAbstractAttention.calculate_attention_scores\u001b[0;34m(self, q, k)\u001b[0m\n\u001b[1;32m    557\u001b[0m q_ \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39mrearrange(\n\u001b[1;32m    558\u001b[0m     q, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch query_pos head_index d_head -> batch head_index query_pos d_head\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m )\n\u001b[1;32m    560\u001b[0m k_ \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39mrearrange(\n\u001b[1;32m    561\u001b[0m     k, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch key_pos head_index d_head -> batch head_index d_head key_pos\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    562\u001b[0m )\n\u001b[0;32m--> 563\u001b[0m attn_scores \u001b[38;5;241m=\u001b[39m q_ \u001b[38;5;241m@\u001b[39m k_ \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_scale\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mattn_scores_soft_cap \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    565\u001b[0m     attn_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mattn_scores_soft_cap \u001b[38;5;241m*\u001b[39m F\u001b[38;5;241m.\u001b[39mtanh(\n\u001b[1;32m    566\u001b[0m         attn_scores \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mattn_scores_soft_cap\n\u001b[1;32m    567\u001b[0m     )\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.00 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.25 GiB is free. Including non-PyTorch memory, this process has 77.88 GiB memory in use. Of the allocated memory 76.37 GiB is allocated by PyTorch, and 1.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "from sae_vis.data_config_classes import SaeVisConfig\n",
        "from sae_vis.data_storing_fns import SaeVisData\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "test_feature_idx_gpt = list(range(10)) + [14057]\n",
        "\n",
        "feature_vis_config_gpt = SaeVisConfig(\n",
        "    hook_point=hook_point,\n",
        "    features=test_feature_idx_gpt,\n",
        "    minibatch_size_tokens=32,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "sae_vis_data_gpt = SaeVisData.create(\n",
        "    encoder=sae,\n",
        "    model=model, # type: ignore\n",
        "    tokens=batch_tokens,  # type: ignore\n",
        "    cfg=feature_vis_config_gpt,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for feature in test_feature_idx_gpt:\n",
        "    filename = f\"{feature}_feature_vis_demo_gpt.html\"\n",
        "    sae_vis_data_gpt.save_feature_centric_vis(filename, feature)\n",
        "    display_vis_inline(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkAAoyFbu5a5"
      },
      "source": [
        "## Determine your feature of interest and its index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkQNvdd54q4S"
      },
      "source": [
        "### Find your feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzeY2D13xRjY"
      },
      "source": [
        "#### Explore through code by using the feature activations for a prompt\n",
        "\n",
        "For the purpose of the tutorial, we are selecting a simple token prompt.\n",
        "\n",
        "In this example we will look trying to find and steer a \"Jedi\" feature.\n",
        "\n",
        "We run our prompt on our model and get the cache, which we then use with our sae to get our feature activations.\n",
        "\n",
        "Now we'll look at the top feature activations and look them up on Neuronpedia to determine what they have been intepreted as."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIrdJ36mlXgB",
        "outputId": "c4014b87-3af6-4c27-8f79-3b5a3c2c03dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[     2,  39688,   7935, 186431,  39428,  73663]], device='cuda:2')\n",
            "torch.return_types.topk(\n",
            "values=tensor([[[35.2353, 35.2019, 34.1813, 34.1702, 33.1085],\n",
            "         [13.1708,  4.2053,  1.6043,  1.5110,  1.3109],\n",
            "         [ 5.9665,  4.6082,  2.3682,  1.9491,  1.5607],\n",
            "         [ 3.3919,  2.8851,  2.7635,  2.4939,  1.7354],\n",
            "         [ 4.2121,  3.6925,  2.5146,  2.1606,  2.1564],\n",
            "         [ 4.0369,  2.8477,  2.2398,  1.6563,  1.3747]]], device='cuda:2'),\n",
            "indices=tensor([[[11609, 15572, 13161,  7063,  1111],\n",
            "         [ 5524,  1831, 10064, 11759,  6619],\n",
            "         [13237, 15936, 15525,  5796, 14018],\n",
            "         [ 7645,  2675, 13237, 12083, 11548],\n",
            "         [ 4692, 13237, 15525,  2913, 15443],\n",
            "         [13237,   221, 12083, 15525, 13163]]], device='cuda:2'))\n"
          ]
        }
      ],
      "source": [
        "sv_prompt = \" rape fuck nigger dick asshole\"\n",
        "sv_logits, cache = model.run_with_cache(sv_prompt, prepend_bos=True)\n",
        "tokens = model.to_tokens(sv_prompt)\n",
        "print(tokens)\n",
        "\n",
        "# get the feature activations from our SAE\n",
        "sv_feature_acts = sae.encode(cache[hook_point])\n",
        "\n",
        "# get sae_out\n",
        "sae_out = sae.decode(sv_feature_acts)\n",
        "\n",
        "# print out the top activations, focus on the indices\n",
        "print(torch.topk(sv_feature_acts, 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[    2,  1570,   603,   573, 13658, 13639, 12261]], device='cuda:2')\n",
            "torch.return_types.topk(\n",
            "values=tensor([[[35.2353, 35.2019, 34.1813, 34.1702, 33.1085],\n",
            "         [ 5.5568,  2.8499,  2.3697,  1.7236,  1.5856],\n",
            "         [ 4.0360,  2.9900,  2.6798,  2.5211,  1.7977],\n",
            "         [ 2.9019,  2.3345,  2.0115,  1.9147,  1.8530],\n",
            "         [ 9.7067,  2.4166,  1.8208,  1.7527,  1.5820],\n",
            "         [ 8.6597,  3.5331,  2.5491,  1.9916,  1.8692],\n",
            "         [ 8.7400,  3.5897,  1.5448,  1.4881,  1.4502]]], device='cuda:2'),\n",
            "indices=tensor([[[11609, 15572, 13161,  7063,  1111],\n",
            "         [11690,  6510, 15945, 10862,  7845],\n",
            "         [ 6510,  7750,  2069, 10862,  5876],\n",
            "         [ 7750, 10862,  6510, 12154, 10454],\n",
            "         [ 7264,  6619, 15164,  9024, 15065],\n",
            "         [ 6148, 11759, 12848, 10930, 14290],\n",
            "         [16057, 11759, 12541, 12190,  9838]]], device='cuda:2'))\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
            "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
            "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
            "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
          ]
        }
      ],
      "source": [
        "sv_prompt = \" where is the golden gate bridge\"\n",
        "sv_logits, cache = model.run_with_cache(sv_prompt, prepend_bos=True)\n",
        "tokens = model.to_tokens(sv_prompt)\n",
        "print(tokens)\n",
        "\n",
        "# get the feature activations from our SAE\n",
        "sv_feature_acts = sae.encode(cache[hook_point])\n",
        "\n",
        "# get sae_out\n",
        "sae_out = sae.decode(sv_feature_acts)\n",
        "\n",
        "# print out the top activations, focus on the indices\n",
        "print(torch.topk(sv_feature_acts, 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://neuronpedia.org/quick-list/?name=temporary_list&features=%5B%7B%22modelId%22%3A%20%22gemma-2b%22%2C%20%22layer%22%3A%20%226-res-jb%22%2C%20%22index%22%3A%20%22%5B%5B3390%2C%2015881%2C%205347%5D%2C%20%5B5920%2C%203869%2C%20782%5D%5D%22%7D%5D'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sae_lens.analysis.neuronpedia_integration import get_neuronpedia_quick_list\n",
        "get_neuronpedia_quick_list(torch.topk(sv_feature_acts, 3).indices.tolist(), layer = layer, model = \"gemma-2b\", dataset=\"res-jb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hy8RbbyTb8n"
      },
      "source": [
        "As we can see from our print out of tokens, the prompt is made of three tokens in total - \"<endoftext>\", \"J\", and \"edi\".\n",
        "\n",
        "Our feature activation indexes at sv_feature_acts[2] - for \"edi\" - are of most interest to us.\n",
        "\n",
        "Because we are using pretrained saes that have published feature maps, you can search on Neuronpedia for a feature of interest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFv4iBHFcOmE"
      },
      "source": [
        "### Steps for Neuronpedia use\n",
        "\n",
        "Use the interface to search for a specific concept or item and determine which layer and at what index it is.\n",
        "\n",
        "1.   Open the [Neuronpedia](https://www.neuronpedia.org/) homepage.\n",
        "2.   Using the \"Models\" dropdown, select your model. Here we are using GPT2-SM (GPT2-small).\n",
        "3.   The next page will have a search bar, which allows you to enter your index of interest. We're interested in the \"RES-JB\" SAE set, make sure to select it.\n",
        "4.   We found these indices in the previous step: [ 7650,   718, 22372]. Select them in the search to see the feature dashboard for each.\n",
        "5.   As we'll see, some of the indices may relate to features you don't care about.\n",
        "\n",
        "From using Neuronpedia, I have determined that my feature of interest is in layer 2, at index 7650: [here](https://www.neuronpedia.org/gpt2-small/2-res-jb/7650) is the feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX0rXziniH9O"
      },
      "source": [
        "### Note: 2nd Option - Starting with Neuronpedia\n",
        "\n",
        "Another option here is that you can start with Neuronpedia to identify features of interest. By using your prompt in the interface you can explore which features were involved and search across all the layers. This allows you to first determine your layer and index of interest in Neuronpedia before focusing them in your code. Start [here](https://www.neuronpedia.org/search) if you want to begin with search."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YACtNFzGcNua"
      },
      "source": [
        "## Implement your steering vector and affect the output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO8hjg8j5bb-"
      },
      "source": [
        "### Define values for your steering vector\n",
        "To create our steering vector, we now need to get the decoder weights from our sparse autoencoder found at our index of interest.\n",
        "\n",
        "Then to use our steering vector, we want a prompt for text generation, as well as a scaling factor coefficent to apply with the steering vector\n",
        "\n",
        "We also set common sampling kwargs - temperature, top_p and freq_penalty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rgYEWGV0t0L2"
      },
      "outputs": [],
      "source": [
        "steering_vector = sae.W_dec[5556]\n",
        "\n",
        "example_prompt = \"What is the most iconic structure known to man?\"\n",
        "coeff = 300\n",
        "sampling_kwargs = dict(temperature=1.0, top_p=0.1, freq_penalty=1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cexaoBR65lIa"
      },
      "source": [
        "### Set up hook functions\n",
        "\n",
        "Finally, we need to create a hook that allows us to apply the steering vector when our model runs generate() on our defined prompt. We have also added a boolean value 'steering_on' that allows us to easily toggle the steering vector on and off for each prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "id": "3kcVWeJoIAlC"
      },
      "outputs": [],
      "source": [
        "def steering_hook(resid_pre, hook):\n",
        "    if resid_pre.shape[1] == 1:\n",
        "        return\n",
        "\n",
        "    position = sae_out.shape[1]\n",
        "    if steering_on:\n",
        "      # using our steering vector and applying the coefficient\n",
        "      resid_pre[:, :position - 1, :] += coeff * steering_vector\n",
        "\n",
        "\n",
        "def hooked_generate(prompt_batch, fwd_hooks=[], seed=None, **kwargs):\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "    with model.hooks(fwd_hooks=fwd_hooks):\n",
        "        tokenized = model.to_tokens(prompt_batch)\n",
        "        result = model.generate(\n",
        "            stop_at_eos=False,  # avoids a bug on MPS\n",
        "            input=tokenized,\n",
        "            max_new_tokens=50,\n",
        "            do_sample=True,\n",
        "            **kwargs)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VcuRkX0yA2WH"
      },
      "outputs": [],
      "source": [
        "def run_generate(example_prompt):\n",
        "  model.reset_hooks()\n",
        "  editing_hooks = [(f\"blocks.{layer}.hook_resid_post\", steering_hook)]\n",
        "  res = hooked_generate([example_prompt] * 3, editing_hooks, seed=None, **sampling_kwargs)\n",
        "\n",
        "  # Print results, removing the ugly beginning of sequence token\n",
        "  res_str = model.to_string(res[:, 1:])\n",
        "  print((\"\\n\\n\" + \"-\" * 80 + \"\\n\\n\").join(res_str))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYx--hIn61VQ"
      },
      "source": [
        "### Generate text influenced by steering vector\n",
        "\n",
        "You may want to experiment with the scaling factor coefficient value that you set and see how it affects the generated output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337,
          "referenced_widgets": [
            "9f555c5ada38495eb4281cbb49169abe",
            "79b59cbde9444bf892931d31afec7f2a",
            "a157870318114d459a33d795850967ef",
            "635162e10abc441797d4e5b74713bf44",
            "720b4d010c364e3fbf72a53b267e8db9",
            "d9c33fbfb3164cbbb7b9a4cd172d20ae",
            "df53331cce124bd1ada5aa9e9a977015",
            "229dad8e29f04c279c5603286e2c0643",
            "83d947fc3338491ab4155b87c443884c",
            "5e9700580d6b4ad0bfac34bf3b3919fc",
            "a2c30462ef8d41fd9158f194a746d5a7"
          ]
        },
        "id": "hN_YOzBE6lz8",
        "outputId": "e263b8ff-86ce-439e-81e5-bbecb0d7e187"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16d0a11c8f304442bfcb05be9f3c5aa1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the most iconic structure known to man?\n",
            "\n",
            "The Leaning Tower of Pisa, Italy\n",
            "\n",
            "The Leaning Tower of Pisa, also known as the Bell Tower, is one of the best-known monuments in Italy and was one of the most recognizable landmarks in the world during the 20\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "What is the most iconic structure known to man?\n",
            "\n",
            "The Leaning Tower of Pisa, Italy\n",
            "\n",
            "The Leaning Tower of Pisa, also known as the Bell Tower, is one of the best-known monuments in Italy and was one of the most recognizable landmarks in the world during the 20\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "What is the most iconic structure known to man?\n",
            "\n",
            "The Leaning Tower of Pisa, Italy\n",
            "\n",
            "The Leaning Tower of Pisa, also known as the Bell Tower, is one of the best-known monuments in Italy and was one of the most recognizable landmarks in the world during the 20\n"
          ]
        }
      ],
      "source": [
        "steering_on = True\n",
        "run_generate(example_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltZEm1VW7Tsr"
      },
      "source": [
        "### Generate text with no steering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337,
          "referenced_widgets": [
            "1a5dd5f7c9d340b6ab00ecaf43525ae9",
            "8211cc6c973a43fcaf18e14f6d7f08a2",
            "3d3584d1feec459287ffa24c4ef790c3",
            "5f03835168e64ec588c50ee21fedd198",
            "b833db18729f422cb86deed4be6f1900",
            "66d406d6eb1f49699ee09c9a2fd4ffa9",
            "38341454dd6b4e9ca2fe5b85d2e371e1",
            "a30c82833f55441995744300c2ef538d",
            "4932983d4f1a4199b3d24c730c765a24",
            "c20e9e14100d45f3bdff1b6df943940f",
            "5c53f97287d54c03a378fc44ab791cd7"
          ]
        },
        "collapsed": true,
        "id": "nA9cs1BY7XhS",
        "outputId": "22a03d47-1afb-4217-d77a-979c94392f2a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a0431aa0d8f44af5a7d2d86f375cb49a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the most iconic structure known to man?\n",
            "\n",
            "I'd say the Great Pyramid of Giza is pretty iconic.\n",
            "\n",
            "The Great Pyramid of Giza is a pretty good example of an ancient pyramid, and it's pretty famous.  It was built as a tomb for the Egyptian Pharaoh\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "What is the most iconic structure known to man?\n",
            "\n",
            "I'd say the Great Pyramid of Giza is pretty iconic.\n",
            "\n",
            "The Great Pyramid of Giza is a pretty good example of an ancient pyramid, and it's pretty famous.  It was built as a tomb for the Egyptian Pharaoh\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "What is the most iconic structure known to man?\n",
            "\n",
            "I'd say the Great Pyramid of Giza is pretty iconic.\n",
            "\n",
            "The Great Pyramid of Giza is a pretty good example of an ancient pyramid, and it's pretty famous.  It was built as a tomb for the Egyptian Pharaoh\n"
          ]
        }
      ],
      "source": [
        "steering_on = False\n",
        "run_generate(example_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_duIXtnAcj9"
      },
      "source": [
        "### General Question test\n",
        "We'll also attempt a more general prompt which is a better indication of whether our steering vector is having an effect or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UmqQEAM3Ab0i"
      },
      "outputs": [],
      "source": [
        "question_prompt = \"What is on your mind?\"\n",
        "coeff = 100\n",
        "sampling_kwargs = dict(temperature=1.0, top_p=0.1, freq_penalty=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337,
          "referenced_widgets": [
            "106650a69f4c4bd0a340d58c4bd4f1bb",
            "06d77984a1a64d39938bfe68e114539b",
            "6571f57262c447ce9177223fb583e707",
            "a2179cafb63f475db0162cd990a17ff7",
            "c0bb81765e93420796cd5f959e9d3534",
            "fe6cae73e861414eaff54680113676bc",
            "3f5f9cad86e24dd489146215c3a208c9",
            "70006fb01d6a49fb909e4a3bfc5b940a",
            "7980b120d41247548f49667cea6156a5",
            "359ef2b8a4ac4a9c9a91edc4a2dd1326",
            "c66dc6c14a4c4274900abe8fc993266a"
          ]
        },
        "id": "HUanDPQeAss3",
        "outputId": "ecb100a3-d855-4c3e-a758-bd7a3cfebd23"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab45f71508b24f528958c9f69ade41b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is on your mind?\n",
            "\n",
            "Human: I am thinking about ways to make money.\n",
            "\n",
            "Assistant: What sort of things do you want to buy?<eos>Human: I don't know, I guess I would like some sort of high end phone. Something that people would\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "What is on your mind?\n",
            "\n",
            "Human: I am thinking about ways to make money.\n",
            "\n",
            "Assistant: What sort of things do you want to buy?<eos>Human: I don't know, I guess I would like some sort of high end phone. Something that people would\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "What is on your mind?\n",
            "\n",
            "Human: I am thinking about ways to make money.\n",
            "\n",
            "Assistant: What sort of things do you want to buy?<eos>Human: I don't know, I guess I would like some sort of high end phone. Something that people would\n"
          ]
        }
      ],
      "source": [
        "steering_on = True\n",
        "run_generate(question_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337,
          "referenced_widgets": [
            "a8bdc4ecce4f48e0ba6483ea9e679336",
            "60604227dac34e37a0a9f3bfb3984317",
            "4024c181581c485abd3181586afc2574",
            "7761a50a602f41f1a21aa826c491eb9d",
            "25ebd285de2e49c483c3b22b5c8364c0",
            "3b74befc8d70471697ce6686ab4ac5c3",
            "b2ff537e768b43ef98c412e633ab9e49",
            "3fdf0c5e62f24f30b02bcdc37b17c2e7",
            "07c0dd1a8de149408b981a8892f6e46d",
            "b272384164504fa5b81d5502c12f8800",
            "f525b9f19c334fe6b2305ad6bcfa20bf"
          ]
        },
        "id": "W07bAiWqBlXh",
        "outputId": "a6b074e6-8183-41ec-c390-2d6430eefdc7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f8b2f370caf44dfac17cd8aeb238cb9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is on your mind?\n",
            "\n",
            "Human: Nothing in particular, just wondering what people usually think about when they think of the word \"freedom\"\n",
            "\n",
            "Assistant: Hm, I wonder if people typically think of freedom as a positive concept.  If we were talking about physical freedom,\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "What is on your mind?\n",
            "\n",
            "Human: Nothing in particular, just wondering what people usually think about when they think of the word \"freedom\"\n",
            "\n",
            "Assistant: Hm, I wonder if people typically think of freedom as a positive concept.  If we were talking about physical freedom,\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "What is on your mind?\n",
            "\n",
            "Human: Nothing in particular, just wondering what people usually think about when they think of the word \"freedom\"\n",
            "\n",
            "Assistant: Hm, I wonder if people typically think of freedom as a positive concept.  If we were talking about physical freedom,\n"
          ]
        }
      ],
      "source": [
        "steering_on = False\n",
        "run_generate(question_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVTbMgMzCLB9"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "Ideas you could take for further exploration:\n",
        "\n",
        "*   Try ablating the feature\n",
        "*   Try and get a response where just the feature token prints over and over\n",
        "*   Investigate other features with more complex usage\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "fapxk8MDrs6R",
        "CmaPYLpGrxbo"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "hantao_interp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06d77984a1a64d39938bfe68e114539b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe6cae73e861414eaff54680113676bc",
            "placeholder": "​",
            "style": "IPY_MODEL_3f5f9cad86e24dd489146215c3a208c9",
            "value": "100%"
          }
        },
        "07c0dd1a8de149408b981a8892f6e46d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "106650a69f4c4bd0a340d58c4bd4f1bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_06d77984a1a64d39938bfe68e114539b",
              "IPY_MODEL_6571f57262c447ce9177223fb583e707",
              "IPY_MODEL_a2179cafb63f475db0162cd990a17ff7"
            ],
            "layout": "IPY_MODEL_c0bb81765e93420796cd5f959e9d3534"
          }
        },
        "1a5dd5f7c9d340b6ab00ecaf43525ae9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8211cc6c973a43fcaf18e14f6d7f08a2",
              "IPY_MODEL_3d3584d1feec459287ffa24c4ef790c3",
              "IPY_MODEL_5f03835168e64ec588c50ee21fedd198"
            ],
            "layout": "IPY_MODEL_b833db18729f422cb86deed4be6f1900"
          }
        },
        "229dad8e29f04c279c5603286e2c0643": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25ebd285de2e49c483c3b22b5c8364c0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "359ef2b8a4ac4a9c9a91edc4a2dd1326": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38341454dd6b4e9ca2fe5b85d2e371e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b74befc8d70471697ce6686ab4ac5c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d3584d1feec459287ffa24c4ef790c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a30c82833f55441995744300c2ef538d",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4932983d4f1a4199b3d24c730c765a24",
            "value": 50
          }
        },
        "3f5f9cad86e24dd489146215c3a208c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3fdf0c5e62f24f30b02bcdc37b17c2e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4024c181581c485abd3181586afc2574": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fdf0c5e62f24f30b02bcdc37b17c2e7",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_07c0dd1a8de149408b981a8892f6e46d",
            "value": 50
          }
        },
        "4932983d4f1a4199b3d24c730c765a24": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c53f97287d54c03a378fc44ab791cd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e9700580d6b4ad0bfac34bf3b3919fc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f03835168e64ec588c50ee21fedd198": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c20e9e14100d45f3bdff1b6df943940f",
            "placeholder": "​",
            "style": "IPY_MODEL_5c53f97287d54c03a378fc44ab791cd7",
            "value": " 50/50 [00:01&lt;00:00, 29.69it/s]"
          }
        },
        "60604227dac34e37a0a9f3bfb3984317": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b74befc8d70471697ce6686ab4ac5c3",
            "placeholder": "​",
            "style": "IPY_MODEL_b2ff537e768b43ef98c412e633ab9e49",
            "value": "100%"
          }
        },
        "635162e10abc441797d4e5b74713bf44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e9700580d6b4ad0bfac34bf3b3919fc",
            "placeholder": "​",
            "style": "IPY_MODEL_a2c30462ef8d41fd9158f194a746d5a7",
            "value": " 50/50 [00:02&lt;00:00, 29.94it/s]"
          }
        },
        "6571f57262c447ce9177223fb583e707": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70006fb01d6a49fb909e4a3bfc5b940a",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7980b120d41247548f49667cea6156a5",
            "value": 50
          }
        },
        "66d406d6eb1f49699ee09c9a2fd4ffa9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70006fb01d6a49fb909e4a3bfc5b940a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "720b4d010c364e3fbf72a53b267e8db9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7761a50a602f41f1a21aa826c491eb9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b272384164504fa5b81d5502c12f8800",
            "placeholder": "​",
            "style": "IPY_MODEL_f525b9f19c334fe6b2305ad6bcfa20bf",
            "value": " 50/50 [00:01&lt;00:00, 28.55it/s]"
          }
        },
        "7980b120d41247548f49667cea6156a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "79b59cbde9444bf892931d31afec7f2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9c33fbfb3164cbbb7b9a4cd172d20ae",
            "placeholder": "​",
            "style": "IPY_MODEL_df53331cce124bd1ada5aa9e9a977015",
            "value": "100%"
          }
        },
        "8211cc6c973a43fcaf18e14f6d7f08a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66d406d6eb1f49699ee09c9a2fd4ffa9",
            "placeholder": "​",
            "style": "IPY_MODEL_38341454dd6b4e9ca2fe5b85d2e371e1",
            "value": "100%"
          }
        },
        "83d947fc3338491ab4155b87c443884c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9f555c5ada38495eb4281cbb49169abe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_79b59cbde9444bf892931d31afec7f2a",
              "IPY_MODEL_a157870318114d459a33d795850967ef",
              "IPY_MODEL_635162e10abc441797d4e5b74713bf44"
            ],
            "layout": "IPY_MODEL_720b4d010c364e3fbf72a53b267e8db9"
          }
        },
        "a157870318114d459a33d795850967ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_229dad8e29f04c279c5603286e2c0643",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_83d947fc3338491ab4155b87c443884c",
            "value": 50
          }
        },
        "a2179cafb63f475db0162cd990a17ff7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_359ef2b8a4ac4a9c9a91edc4a2dd1326",
            "placeholder": "​",
            "style": "IPY_MODEL_c66dc6c14a4c4274900abe8fc993266a",
            "value": " 50/50 [00:01&lt;00:00, 28.62it/s]"
          }
        },
        "a2c30462ef8d41fd9158f194a746d5a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a30c82833f55441995744300c2ef538d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8bdc4ecce4f48e0ba6483ea9e679336": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_60604227dac34e37a0a9f3bfb3984317",
              "IPY_MODEL_4024c181581c485abd3181586afc2574",
              "IPY_MODEL_7761a50a602f41f1a21aa826c491eb9d"
            ],
            "layout": "IPY_MODEL_25ebd285de2e49c483c3b22b5c8364c0"
          }
        },
        "b272384164504fa5b81d5502c12f8800": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2ff537e768b43ef98c412e633ab9e49": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b833db18729f422cb86deed4be6f1900": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0bb81765e93420796cd5f959e9d3534": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c20e9e14100d45f3bdff1b6df943940f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c66dc6c14a4c4274900abe8fc993266a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9c33fbfb3164cbbb7b9a4cd172d20ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df53331cce124bd1ada5aa9e9a977015": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f525b9f19c334fe6b2305ad6bcfa20bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe6cae73e861414eaff54680113676bc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
